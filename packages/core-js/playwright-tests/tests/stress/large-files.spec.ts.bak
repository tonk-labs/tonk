import { test, expect } from '../fixtures';
import { setupTestWithServer, waitForVFSConnection } from '../fixtures';
import { imageGenerator } from '../../src/utils/image-generator';
import { MetricsCollector } from '../../src/utils/metrics-collector';

test.describe('Large File Handling Tests', () => {
  test('should handle files up to 100MB efficiently', async ({
    page,
    serverInstance,
  }) => {
    await setupTestWithServer(page, serverInstance);
    await waitForVFSConnection(page);

    const fileSizes = [10, 25, 50, 75, 100]; // MB
    const results: Array<{
      sizeMB: number;
      throughputMBps: number;
      latencyMs: number;
    }> = [];

    console.log(
      `Testing large file handling: ${fileSizes.join(', ')} MB files`
    );

    for (const sizeMB of fileSizes) {
      console.log(`Testing ${sizeMB}MB file...`);

      const metricsCollector = new MetricsCollector(`large-file-${sizeMB}MB`);
      const image = (
        await imageGenerator.generateBatchForTest(`large-file-${sizeMB}MB`, 1, [
          sizeMB,
          sizeMB,
        ])
      )[0];

      const startTime = Date.now();
      const endOperation = metricsCollector.startOperation();

      try {
        // Upload large file
        await page.evaluate(
          async (params: { imageData: number[]; imageName: string }) => {
            const vfsService = (window as any).vfsService;
            if (!vfsService) throw new Error('VFS service not available');

            await vfsService.writeFile(
              params.imageName,
              new Uint8Array(params.imageData)
            );
          },
          {
            imageData: Array.from(image.data),
            imageName: `large-${sizeMB}MB.jpg`,
          }
        );

        endOperation();
        metricsCollector.recordBytes(image.size);

        const duration = (Date.now() - startTime) / 1000;
        const throughputMBps = sizeMB / duration;

        // Read back to verify
        const readStartTime = Date.now();
        const readContent = await page.evaluate(
          async (params: { fileName: string }) => {
            const vfsService = (window as any).vfsService;
            return await vfsService.readFile(params.fileName);
          },
          { fileName: `large-${sizeMB}MB.jpg` }
        );

        const readDuration = (Date.now() - readStartTime) / 1000;
        const readThroughputMBps = sizeMB / readDuration;

        // Verify file integrity
        expect((readContent as Uint8Array).length).toBe(image.size);

        const metrics = await metricsCollector.getMetrics();

        results.push({
          sizeMB: sizeMB,
          throughputMBps: throughputMBps,
          latencyMs: metrics.latency.mean,
        });

        console.log(
          `${sizeMB}MB: Write ${throughputMBps.toFixed(1)} MB/s, Read ${readThroughputMBps.toFixed(1)} MB/s, Latency ${metrics.latency.mean.toFixed(0)}ms`
        );

        // Individual file assertions
        expect(throughputMBps).toBeGreaterThan(1); // At least 1 MB/s write
        expect(readThroughputMBps).toBeGreaterThan(1); // At least 1 MB/s read
        expect(metrics.latency.mean).toBeLessThan(sizeMB * 1000); // Reasonable latency scaling
      } catch (error) {
        console.error(`Failed to handle ${sizeMB}MB file:`, error);
        throw error;
      }
    }

    // Overall analysis
    console.log('Large file handling results:', results);

    // All files should be handled successfully
    expect(results.length).toBe(fileSizes.length);

    // Performance should remain reasonable even for largest files
    const largestFile = results[results.length - 1];
    expect(largestFile.throughputMBps).toBeGreaterThan(0.5); // At least 0.5 MB/s for 100MB
    expect(largestFile.latencyMs).toBeLessThan(120000); // Less than 2 minutes for 100MB
  }, 300000); // 5 minute timeout

  test('should maintain memory efficiency with large files', async ({
    page,
    serverInstance,
  }) => {
    await setupTestWithServer(page, serverInstance);
    await waitForVFSConnection(page);

    const fileSize = 50; // MB
    const fileCount = 5;

    console.log(
      `Testing memory efficiency: ${fileCount} files of ${fileSize}MB each`
    );

    const metricsCollector = new MetricsCollector(
      'memory-efficiency-large-files'
    );
    const memorySnapshots: Array<{ file: number; heapUsed: number }> = [];

    // Baseline memory
    const baselineMemory = await metricsCollector.collectMemoryMetrics();
    memorySnapshots.push({ file: 0, heapUsed: baselineMemory.heapUsed });

    for (let i = 1; i <= fileCount; i++) {
      console.log(`Processing file ${i}/${fileCount}...`);

      const image = (
        await imageGenerator.generateBatchForTest(
          `memory-efficiency-${fileSize}MB-file${i}`,
          1,
          [fileSize, fileSize]
        )
      )[0];
      const endOperation = metricsCollector.startOperation();

      try {
        // Write large file
        await page.evaluate(
          async (params: { imageData: number[]; imageName: string }) => {
            const vfsService = (window as any).vfsService;
            await vfsService.writeFile(
              params.imageName,
              new Uint8Array(params.imageData)
            );
          },
          {
            imageData: Array.from(image.data),
            imageName: `memory-test-${i}.jpg`,
          }
        );

        endOperation();
        metricsCollector.recordBytes(image.size);

        // Memory snapshot after each file
        const currentMemory = await metricsCollector.collectMemoryMetrics();
        memorySnapshots.push({ file: i, heapUsed: currentMemory.heapUsed });

        const memoryIncreaseMB =
          (currentMemory.heapUsed - baselineMemory.heapUsed) / (1024 * 1024);
        console.log(
          `After file ${i}: Heap increased by ${memoryIncreaseMB.toFixed(1)}MB`
        );

        // Memory shouldn't grow excessively with each large file
        expect(memoryIncreaseMB).toBeLessThan(fileSize * i * 2); // No more than 2x the data size
      } catch (error) {
        endOperation();
        console.error(`Failed on file ${i}:`, error);
        throw error;
      }
    }

    // Final memory analysis
    const finalMemory = memorySnapshots[memorySnapshots.length - 1];
    const totalMemoryIncreaseMB =
      (finalMemory.heapUsed - baselineMemory.heapUsed) / (1024 * 1024);

    console.log(`Memory efficiency test completed:`);
    console.log(`- Files processed: ${fileCount} x ${fileSize}MB`);
    console.log(`- Total data: ${fileCount * fileSize}MB`);
    console.log(`- Memory increase: ${totalMemoryIncreaseMB.toFixed(1)}MB`);
    console.log(
      `- Memory efficiency: ${((fileCount * fileSize) / totalMemoryIncreaseMB).toFixed(1)}x`
    );

    // Memory efficiency assertions
    expect(totalMemoryIncreaseMB).toBeLessThan(fileCount * fileSize * 1.5); // Less than 1.5x data size
    expect(totalMemoryIncreaseMB).toBeLessThan(500); // Less than 500MB total increase
  }, 300000); // 5 minute timeout

  test('should handle streaming large files without blocking', async ({
    page,
    serverInstance,
  }) => {
    await setupTestWithServer(page, serverInstance);
    await waitForVFSConnection(page);

    const fileSize = 30; // MB
    const simultaneousFiles = 3;

    console.log(
      `Testing streaming: ${simultaneousFiles} files of ${fileSize}MB simultaneously`
    );

    const metricsCollector = new MetricsCollector('streaming-large-files');
    const startTime = Date.now();

    // Generate test files
    const images = await imageGenerator.generateBatchForTest(
      `streaming-${simultaneousFiles}-files-${fileSize}MB`,
      simultaneousFiles,
      [fileSize, fileSize]
    );

    // Upload files simultaneously to test streaming
    const uploadPromises = images.map(async (image, index) => {
      const fileStartTime = Date.now();
      const endOperation = metricsCollector.startOperation();

      try {
        await page.evaluate(
          async (params: { imageData: number[]; imageName: string }) => {
            const vfsService = (window as any).vfsService;
            await vfsService.writeFile(
              params.imageName,
              new Uint8Array(params.imageData)
            );
          },
          {
            imageData: Array.from(image.data),
            imageName: `streaming-${index}.jpg`,
          }
        );

        const fileDuration = (Date.now() - fileStartTime) / 1000;
        const fileThroughput = fileSize / fileDuration;

        endOperation();
        metricsCollector.recordBytes(image.size);

        console.log(
          `File ${index} completed: ${fileThroughput.toFixed(1)} MB/s in ${fileDuration.toFixed(1)}s`
        );

        return { index, duration: fileDuration, throughput: fileThroughput };
      } catch (error) {
        endOperation();
        console.error(`File ${index} failed:`, error);
        throw error;
      }
    });

    // Wait for all uploads to complete
    const results = await Promise.all(uploadPromises);
    const totalDuration = (Date.now() - startTime) / 1000;
    const totalData = fileSize * simultaneousFiles;
    const overallThroughput = totalData / totalDuration;

    console.log(`Streaming test completed:`);
    console.log(`- Total duration: ${totalDuration.toFixed(1)}s`);
    console.log(`- Total data: ${totalData}MB`);
    console.log(`- Overall throughput: ${overallThroughput.toFixed(1)} MB/s`);
    console.log(
      `- Concurrent efficiency: ${(overallThroughput / results[0].throughput).toFixed(1)}x`
    );

    // Streaming assertions
    expect(overallThroughput).toBeGreaterThan(1); // At least 1 MB/s overall
    expect(totalDuration).toBeLessThan(fileSize * simultaneousFiles * 2); // Should be faster than sequential

    // All files should complete successfully
    expect(results.length).toBe(simultaneousFiles);
    results.forEach((result, index) => {
      expect(result.index).toBe(index);
      expect(result.throughput).toBeGreaterThan(0.5); // Each file at least 0.5 MB/s
    });
  }, 300000); // 5 minute timeout
});
